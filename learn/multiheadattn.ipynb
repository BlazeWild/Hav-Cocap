{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3f1586",
   "metadata": {},
   "source": [
    "# MULTI HEAD ATTENTION MATRIX OPERATIONS in DETAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d31c1",
   "metadata": {},
   "source": [
    "## 1.Setup and Input Tensors\n",
    "First, we define our inputs. In NLP, the input shape is typically $(B, S, N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2d7e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: \n",
      "[[[-0.78  0.19  1.48 -1.23]\n",
      "  [ 1.96  0.47 -0.4   0.59]\n",
      "  [-0.1  -0.76  0.97  1.61]\n",
      "  [ 1.63 -0.93  3.04 -0.44]\n",
      "  [-1.11  1.49 -0.39 -0.31]\n",
      "  [-1.87  0.83 -0.9   0.39]]\n",
      "\n",
      " [[ 0.68 -1.06  0.47 -0.77]\n",
      "  [ 0.64 -1.77 -0.7  -2.07]\n",
      "  [-0.54 -0.03 -1.02 -0.56]\n",
      "  [-1.35 -0.49 -0.42 -0.7 ]\n",
      "  [-0.03  0.45 -0.36  0.96]\n",
      "  [-0.23 -0.91  1.46  0.07]]]\n",
      " and shape: (2, 6, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#dimensions \n",
    "S=6\n",
    "B=2\n",
    "N=4\n",
    "H=2\n",
    "d_head = N//H\n",
    "\n",
    "\n",
    "#create random input X\n",
    "X =np.random.randn(B,S,N).round(2)\n",
    "\n",
    "print(f\"Input X: \\n{X}\\n and shape: {X.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36b75737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so to write 2,6,4 it is liek this\n",
    "D = np.array([\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.1, 0.2, 0.3, 0.4]\n",
    "    ],\n",
    "    [\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.5, 0.6, 0.7, 0.8]\n",
    "    ]\n",
    "])\n",
    "\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f1b6f",
   "metadata": {},
   "source": [
    "### 2. Weight Matrices Initialization\n",
    "In MHA, we project our input into Queries ($Q$), Keys ($K$), and Values ($V$). Each projection has its own weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "018b90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q: \n",
      "[[ 0.1   0.27 -0.35 -0.11]\n",
      " [-1.86 -0.25 -1.25 -0.36]\n",
      " [-0.71  1.71 -1.17  1.05]\n",
      " [ 0.08  1.76  0.53 -0.1 ]]\n",
      " W_k: \n",
      "[[ 0.7  -1.52  2.67  0.06]\n",
      " [-1.46  2.02 -0.78 -0.83]\n",
      " [ 1.39 -1.07 -0.52  1.11]\n",
      " [ 1.73  1.05  1.47  1.17]]\n",
      " W_v: \n",
      "[[-1.26  0.25  0.03  1.92]\n",
      " [ 0.58 -0.25 -0.46  1.69]\n",
      " [ 0.08 -1.85 -0.71  0.3 ]\n",
      " [ 1.64 -0.9   1.04  0.72]]\n",
      "\n",
      "W_q shape: (4, 4), W_k shape: (4, 4), W_v shape: (4, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weights for Q,K,V\n",
    "# shape: (In_features, out_features) -> (N, N)\n",
    "W_q = np.random.randn(N, N).round(2)\n",
    "W_k = np.random.randn(N, N).round(2)\n",
    "W_v = np.random.randn(N, N).round(2)\n",
    "\n",
    "print(f\"W_q: \\n{W_q}\\n W_k: \\n{W_k}\\n W_v: \\n{W_v}\\n\")\n",
    "print(f\"W_q shape: {W_q.shape}, W_k shape: {W_k.shape}, W_v shape: {W_v.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b131c3d",
   "metadata": {},
   "source": [
    "### 3: Linear Projections (Q, K, V Calculation)\n",
    "\n",
    "We multiply the input $X$ by the weights. Note that the multiplication happens on the last dimension.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Q_total: shape (B=2, S=3, N=4)\n",
    "# B=Batch, S=Sequence, N=Features\n",
    "Q_total = np.array([\n",
    "    [   # B=0\n",
    "        [-1.5116,  1.5644,  2.1816,  3.3755],  # S=0\n",
    "        [-2.6180,  2.2626,  1.1976,  1.1208],  # S=1\n",
    "        [-3.0033,  0.1768,  0.5522, -0.1006]   # S=2\n",
    "    ],\n",
    "    [   # B=1\n",
    "        [-1.7886,  3.2338,  0.2871, -0.4054],  # S=0\n",
    "        [ 1.3256, -2.1606,  0.2825, -0.0161],  # S=1\n",
    "        [ 0.7166, -0.7093, -0.7909, -1.1111]   # S=2\n",
    "    ]\n",
    "])\n",
    "\n",
    "# W: shape (N=4, N=4)\n",
    "W = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6]\n",
    "])\n",
    "\n",
    "# Multiply: (B,S,N) @ (N,N) → (B,S,N)\n",
    "output = np.matmul(Q_total, W)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(output)\n",
    "\n",
    "```\n",
    "\n",
    "**What happens internally:**\n",
    "For **B=0, S=0**:\n",
    "\n",
    "```python\n",
    "# shape (4,)\n",
    "[-1.5116, 1.5644, 2.1816, 3.3755] @ W\n",
    "# = [\n",
    "#   -1.5116*0.1 + 1.5644*0.5 + 2.1816*0.9 + 3.3755*1.3,\n",
    "#   -1.5116*0.2 + 1.5644*0.6 + 2.1816*1.0 + 3.3755*1.4,\n",
    "#   -1.5116*0.3 + 1.5644*0.7 + 2.1816*1.1 + 3.3755*1.5,\n",
    "#   -1.5116*0.4 + 1.5644*0.8 + 2.1816*1.2 + 3.3755*1.6\n",
    "# ]\n",
    "\n",
    "```\n",
    "\n",
    "For **B=1, S=0**:\n",
    "\n",
    "```python\n",
    "[-1.7886,  3.2338,  0.2871, -0.4054] @ W → (4,)\n",
    "\n",
    "```\n",
    "\n",
    "…and similarly for all **B** and **S**.\n",
    "\n",
    "#### ✅ Output\n",
    "\n",
    "**Shape:** `(2, 3, 4)` — same as input except last dimension transformed by W.\n",
    "\n",
    "Visual 3D array structure:\n",
    "\n",
    "```text\n",
    "[\n",
    "  [ [ ...4 elements... ], [ ...4 elements... ], [ ...4 elements... ] ],   # B=0\n",
    "  [ [ ...4 elements... ], [ ...4 elements... ], [ ...4 elements... ] ]    # B=1\n",
    "]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc93a985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_total: \n",
      "[[[-1.5806  0.1079 -2.348   1.6944]\n",
      "  [-0.347   0.7661 -0.4928 -0.8638]\n",
      "  [ 0.8437  4.6553  0.7034  1.1421]\n",
      "  [-0.3008  5.0966 -3.198   3.3915]\n",
      "  [-2.6303 -1.8847 -1.182  -0.7928]\n",
      "  [-1.0606 -1.565   0.8767 -1.0771]]\n",
      "\n",
      " [[ 1.6443 -0.1029  0.129   0.8773]\n",
      "  [ 3.6876 -4.2249  1.7104  0.0388]\n",
      "  [ 0.6812 -2.8681  1.1231 -0.9448]\n",
      "  [ 1.0186 -2.1922  1.2054 -0.0461]\n",
      "  [-0.5076  0.9534  0.378  -0.6327]\n",
      "  [ 0.6386  2.7852 -0.4531  1.8789]]]\n",
      " and shape: (2, 6, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matrix:multiply: (S,B,N) @ (N,N) -> (S,B,N)\n",
    "\n",
    "Q_total = np.matmul(X,W_q)\n",
    "K_total = np.matmul(X,W_k)\n",
    "V_total = np.matmul(X,W_v)\n",
    "\n",
    "print(f\"Q_total: \\n{Q_total}\\n and shape: {Q_total.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc0e08",
   "metadata": {},
   "source": [
    "To adapt the layout to **(Batch, Sequence, Features)** or `(B, S, N)`, we reorganize the indices so that each primary group represents a full batch item rather than a sequence step.\n",
    "\n",
    "### NumPy Matrix Multiplication with 3D Array (Batch + Sequence)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Q_total: shape (B=2, S=3, N=4)\n",
    "Q_total = np.array([\n",
    "    [   # B=0\n",
    "        [-1.5116,  1.5644,  2.1816,  3.3755],  # S=0\n",
    "        [-2.6180,  2.2626,  1.1976,  1.1208],  # S=1\n",
    "        [-3.0033,  0.1768,  0.5522, -0.1006]   # S=2\n",
    "    ],\n",
    "    [   # B=1\n",
    "        [-1.7886,  3.2338,  0.2871, -0.4054],  # S=0\n",
    "        [ 1.3256, -2.1606,  0.2825, -0.0161],  # S=1\n",
    "        [ 0.7166, -0.7093, -0.7909, -1.1111]   # S=2\n",
    "    ]\n",
    "])\n",
    "\n",
    "# W: shape (N=4, N=4)\n",
    "W = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6]\n",
    "])\n",
    "\n",
    "# Matrix multiplication: (B,S,N) @ (N,N) → (B,S,N)\n",
    "output = np.matmul(Q_total, W)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "#### What happens internally?\n",
    "\n",
    "For each **batch item (B)** and each **sequence position (S)**, NumPy performs a vector-matrix multiplication:\n",
    "\n",
    "```python\n",
    "# Example: B=0, S=0   shape (4,) @ (4,4) → (4,)\n",
    "[-1.5116, 1.5644, 2.1816, 3.3755] @ W\n",
    "\n",
    "# Which computes:\n",
    "[\n",
    "  -1.5116*0.1 + 1.5644*0.5 + 2.1816*0.9 + 3.3755*1.3,\n",
    "  -1.5116*0.2 + 1.5644*0.6 + 2.1816*1.0 + 3.3755*1.4,\n",
    "  -1.5116*0.3 + 1.5644*0.7 + 2.1816*1.1 + 3.3755*1.5,\n",
    "  -1.5116*0.4 + 1.5644*0.8 + 2.1816*1.2 + 3.3755*1.6\n",
    "]\n",
    "```\n",
    "\n",
    "The same operation is applied independently to **every** combination of B and S.\n",
    "\n",
    "#### Result\n",
    "\n",
    "* **Input shape**: `(2, 3, 4)` → B=2 (batch), S=3 (sequence), N=4 (features)\n",
    "* **Output shape**: `(2, 3, 4)` — same shape, last dimension transformed by W\n",
    "\n",
    "#### Visual structure of the 3D array\n",
    "\n",
    "```text\n",
    "[\n",
    "  [ [ ...N=4... ], [ ...N=4... ], [ ...N=4... ] ],   # ← B = 0\n",
    "  [ [ ...N=4... ], [ ...N=4... ], [ ...N=4... ] ]    # ← B = 1\n",
    "]\n",
    "      ↑                ↑                ↑\n",
    "    Seq 0            Seq 1            Seq 2\n",
    "\n",
    "```\n",
    "\n",
    "Perfect for transformer-style operations where you apply the same weight matrix to every token in every batch item!\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690e137",
   "metadata": {},
   "source": [
    "### 4.Splitting Heads (The B, S, N way)\n",
    "This is where it gets interesting. To keep things clean, we want the heads to be in a dimension where they don't interfere with the Sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96d4c05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after head split: (2, 2, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "def split_heads(tensor, B, S, H, d_head):\n",
    "    # 1. Reshape: (B, S, N) -> (B, S, H, d_head)\n",
    "    tensor = tensor.reshape(B, S, H, d_head)\n",
    "    # 2. Transpose: (B, S, H, d_head) -> (B, H, S, d_head)\n",
    "    return tensor.transpose(0, 2, 1, 3)\n",
    "\n",
    "Q = split_heads(Q_total, B, S, H, d_head)\n",
    "K = split_heads(K_total, B, S, H, d_head)\n",
    "V = split_heads(V_total, B, S, H, d_head)\n",
    "\n",
    "print(f\"Shape after head split: {Q.shape}\") \n",
    "# Result: (2, 2, 6, 2) -> (Batch, Heads, Seq_Len, Head_Dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f195a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 6, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy Q is\n",
    "Q_d = np.array([\n",
    "    [ # first batch\n",
    "        [ #first head\n",
    "            [1,1], [1,1], [1,1], [1,1], [1,1], [1,1] #seq_len=6 and each seq has 2 values of dim=2\n",
    "        ],\n",
    "        [# for second head \n",
    "            [2,2], [2,2], [2,2], [2,2], [2,2], [2,2] #seq len=6\n",
    "        ]\n",
    "    ],\n",
    "    [# second batch\n",
    "        [ # first head\n",
    "            [3,3], [3,3], [3,3], [3,3], [3,3], [3,3] #seq_len=6\n",
    "        ],\n",
    "        [ # second head\n",
    "            [4,4], [4,4], [4,4], [4,4], [4,4], [4,4]#seq_len=6\n",
    "        ]\n",
    "    ]\n",
    "])\n",
    "\n",
    "Q_d.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933875f",
   "metadata": {},
   "source": [
    "### 5: Scaled Dot-Product AttentionWe \n",
    "calculate the similarity between $Q$ and $K$.Formula: $Attention(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38758e3",
   "metadata": {},
   "source": [
    " ##### 5.1: The Raw Scores (Dot Product)\n",
    "\n",
    "In this step, we calculate the similarity between every word's Query and every word's Key.\n",
    "\n",
    "**Math:** $Q \\cdot K^T$  \n",
    "\n",
    "**Shape:** $(B, H, S, d_{head}) \\times (B, H, d_{head}, S) \\rightarrow (B, H, S, S)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07973b",
   "metadata": {},
   "source": [
    "more explanation on \n",
    " Why Transpose `(B, S, H, d_{head}) → (B, H, S, d_{head})` and Why K is Transposed\n",
    "\n",
    "1. **After splitting heads (paper convention)**:\n",
    "\n",
    "- Suppose the input embedding is `(B, S, N)` with `N = H * d_{head}`.  \n",
    "- After splitting into heads:  \n",
    "  $$\n",
    "  (B, S, N) \\rightarrow (B, S, H, d_{head})\n",
    "  $$  \n",
    "  This is the **paper convention**, where:  \n",
    "  - `B` = batch size  \n",
    "  - `S` = sequence length  \n",
    "  - `H` = number of heads  \n",
    "  - `d_{head}` = dimension per head  \n",
    "\n",
    "2. **Why transpose to `(B, H, S, d_{head})`?**\n",
    "\n",
    "- Attention is **computed independently for each head**, so we move `H` upfront to make operations **vectorized across heads**.  \n",
    "- This makes it easy to perform batch matrix multiplication for all heads simultaneously:\n",
    "\n",
    "  $$\n",
    "  Q: (B, H, S, d_{head}), \\quad K: (B, H, S, d_{head})\n",
    "  $$\n",
    "\n",
    "3. **Why do we transpose K to get Kᵀ?**\n",
    "\n",
    "- The attention score is a **dot product between Q and K for every token pair**. For a single head:  \n",
    "\n",
    "  - Q has shape `(S, d_head)`  \n",
    "  - K has shape `(S, d_head)`  \n",
    "  - To get **all pairwise scores** between tokens, we do:  \n",
    "    $$\n",
    "    Q \\cdot K^T \\quad \\text{→ shape: } (S, S)\n",
    "    $$\n",
    "\n",
    "- In the batched, multi-head form:\n",
    "\n",
    "  $$\n",
    "  Q: (B, H, S, d_{head}), \\quad K^T: (B, H, d_{head}, S) \\\\\n",
    "  Q \\cdot K^T \\rightarrow (B, H, S, S)\n",
    "  $$\n",
    "\n",
    "- Transposing K along the last two dimensions is necessary because **matrix multiplication in attention sums over the embedding dimension** (`d_head`) and produces a score for **each pair of tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "| Step | Shape | Reason |\n",
    "|------|-------|--------|\n",
    "| Split heads | `(B, S, H, d_head)` | Paper convention, separate heads |\n",
    "| Transpose to `(B, H, S, d_head)` | `(B, H, S, d_head)` | Put heads upfront for vectorized attention computation |\n",
    "| Compute Q · Kᵀ | `(B, H, S, S)` | Each head computes **pairwise attention scores** over sequence positions; transpose K so dot product sums over d_head |\n",
    "\n",
    "**Key Idea:**  \n",
    "- `(B, S, H, d_head)` → `(B, H, S, d_head)` makes computation **batch-friendly and head-friendly**.  \n",
    "- Kᵀ is needed so the **dot product sums over the head dimension** to produce scores for every token pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fa3dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores shape: (2, 2, 6, 6)\n",
      " Scores: \n",
      "[[[[ 1.27232943e+00 -1.92447047e+00 -8.25559166e+00 -1.02962948e+01\n",
      "     6.88779400e+00  5.53096911e+00]\n",
      "   [-6.90044070e-01 -1.15176353e+00 -2.35481306e+00 -8.25242350e+00\n",
      "     5.06738728e+00  5.58817185e+00]\n",
      "   [-6.83277738e+00 -3.60222434e+00  9.63466660e-01 -3.25417745e+01\n",
      "     1.88925147e+01  2.48136767e+01]\n",
      "   [-6.38568534e+00 -5.35246058e+00 -5.27967452e+00 -4.29283892e+01\n",
      "     2.56190627e+01  3.09582266e+01]\n",
      "   [ 4.81260402e+00 -1.17481934e+00 -1.22301061e+01 -4.73591260e-01\n",
      "     1.57676188e+00 -2.95740792e+00]\n",
      "   [ 2.99170296e+00  3.17079200e-01 -4.34330692e+00  6.30654508e+00\n",
      "    -3.21940552e+00 -5.93541324e+00]]\n",
      "\n",
      "  [[ 1.12890025e+01 -1.39959745e+01  9.37104080e-01 -3.72639280e-01\n",
      "     6.72504624e+00  8.52151728e+00]\n",
      "   [ 2.37031984e+00 -2.90553676e+00 -4.17519144e+00 -4.62580006e+00\n",
      "     3.97090246e+00  3.42716164e+00]\n",
      "   [-3.38321258e+00  4.14960944e+00  5.63376646e+00  6.26385319e+00\n",
      "    -5.47720161e+00 -4.76969058e+00]\n",
      "   [ 1.53748698e+01 -1.90910535e+01  5.16166200e+00  3.53394075e+00\n",
      "     6.88498305e+00  1.01501043e+01]\n",
      "   [ 5.68428124e+00 -7.00255444e+00 -5.42868076e+00 -6.32517084e+00\n",
      "     6.83974952e+00  6.50138264e+00]\n",
      "   [-4.21475027e+00  5.23748375e+00 -1.94331165e+00 -1.51832170e+00\n",
      "    -1.57816877e+00 -2.58453632e+00]]]\n",
      "\n",
      "\n",
      " [[[ 2.67288462e+00 -1.88786934e+00 -4.60383588e+00 -3.40863306e+00\n",
      "     5.51621700e-01  5.76231600e+00]\n",
      "   [ 2.39128309e+01  1.96219018e+01 -1.53718057e+01 -1.07462348e+01\n",
      "    -8.14032198e+00  2.48146203e+01]\n",
      "   [ 1.37829480e+01  1.60935826e+01 -5.47754012e+00 -3.60638774e+00\n",
      "    -6.40511430e+00  1.07994838e+01]\n",
      "   [ 1.12044609e+01  1.15431456e+01 -5.54147080e+00 -3.76451636e+00\n",
      "    -4.65547452e+00  9.90665372e+00]\n",
      "   [-4.95976356e+00 -4.92185574e+00  2.58579432e+00  1.76799588e+00\n",
      "     1.99352628e+00 -4.52282538e+00]\n",
      "   [-1.16361750e+01 -1.76070494e+01  1.78187584e+00  8.70204480e-01\n",
      "     6.84715320e+00 -6.17343730e+00]]\n",
      "\n",
      "  [[ 6.38297120e-01 -1.43091072e+00 -1.79541057e+00 -1.36201345e+00\n",
      "     4.56145290e-01  2.07180380e+00]\n",
      "   [ 2.18654376e+00  6.36492880e-01 -2.99647860e+00 -6.93510076e+00\n",
      "     2.01006396e+00 -8.64023040e-01]\n",
      "   [ 9.10442190e-01  2.05906727e+00 -2.26027200e-01 -3.62281439e+00\n",
      "     9.81920790e-01 -2.93870106e+00]\n",
      "   [ 1.50119840e+00  5.72790240e-01 -1.97993559e+00 -4.81702471e+00\n",
      "     1.39100679e+00 -7.88415640e-01]\n",
      "   [ 1.36042020e-01  1.22531778e+00  4.88799630e-01 -9.17360550e-01\n",
      "     2.20869990e-01 -1.75822560e+00]\n",
      "   [ 4.43566550e-01 -3.36396901e+00 -2.59709289e+00  2.45024400e-02\n",
      "     1.25517240e-01  4.84603946e+00]]]]\n",
      "\n",
      "Raw Score Matrix (Batch 0, Head 0):\n",
      " [[  1.27  -1.92  -8.26 -10.3    6.89   5.53]\n",
      " [ -0.69  -1.15  -2.35  -8.25   5.07   5.59]\n",
      " [ -6.83  -3.6    0.96 -32.54  18.89  24.81]\n",
      " [ -6.39  -5.35  -5.28 -42.93  25.62  30.96]\n",
      " [  4.81  -1.17 -12.23  -0.47   1.58  -2.96]\n",
      " [  2.99   0.32  -4.34   6.31  -3.22  -5.94]]\n"
     ]
    }
   ],
   "source": [
    "# 1. We need to flip the last two dimension of K to do the dot product \n",
    "K_T = K.transpose(0,1,3,2)  # (B, H, d_head, S)\n",
    "\n",
    "# matrix multiplication\n",
    "# resulting shape : (2,2,6,6) -> (Batch, Heads, Seq_Len, Seq_Len)\n",
    "scores = np.matmul(Q, K_T)\n",
    "\n",
    "print(f\"Scores shape: {scores.shape}\\n Scores: \\n{scores}\\n\")\n",
    "print(\"Raw Score Matrix (Batch 0, Head 0):\\n\", scores[0, 0].round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224e7d1",
   "metadata": {},
   "source": [
    "##### 5.2 Scaling for Gradient Stability\n",
    "\n",
    "We divide by $\\sqrt{d_{head}}$ ($d_k$ in the paper) to prevent attention scores from becoming too large, which would cause the softmax gradients to vanish.\n",
    "\n",
    "**Proper explanation:**\n",
    "\n",
    "1. **Dot product grows with vector length**\n",
    "\n",
    "   Each attention score is computed as the dot product between a Query and a Key vector:\n",
    "\n",
    "   $$\n",
    "   Q \\cdot K = \\sum_{i=1}^{d_{head}} Q_i K_i\n",
    "   $$\n",
    "\n",
    "   - If the elements $Q_i$ and $K_i$ are independent with **mean 0** and **variance $\\sigma^2$**, the **expected squared magnitude** of the dot product is proportional to $d_{head}$:\n",
    "\n",
    "   $$\n",
    "   \\text{Var}(Q \\cdot K) = \\sum_{i=1}^{d_{head}} \\text{Var}(Q_i K_i) \\approx d_{head} \\cdot \\sigma^2\n",
    "   $$\n",
    "\n",
    "   - This shows that **as $d_{head}$ increases, the magnitude of the raw scores grows**.  \n",
    "\n",
    "2. **Effect on softmax**\n",
    "\n",
    "   The softmax function is sensitive to the scale of its inputs:\n",
    "\n",
    "   $$\n",
    "   \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "   $$\n",
    "\n",
    "   - Large values in $x_i$ → softmax becomes **very peaked** (one value ≈ 1, others ≈ 0).  \n",
    "   - Small gradients → **vanishing gradients** during backpropagation.  \n",
    "\n",
    "3. **Scaling by \\(\\sqrt{d_{head}}\\)**\n",
    "\n",
    "   To normalize the scale of the dot product, we divide by $\\sqrt{d_{head}}$:\n",
    "\n",
    "   $$\n",
    "   \\text{Scaled Score} = \\frac{Q \\cdot K}{\\sqrt{d_{head}}}\n",
    "   $$\n",
    "\n",
    "   - Why $\\sqrt{d_{head}}$ and not $d_{head}$?  \n",
    "     - Dividing by $d_{head}$ would over-shrink the scores. Softmax would become **too flat**, making the model insensitive to differences in attention.  \n",
    "     - Dividing by $\\sqrt{d_{head}}$ **normalizes the variance** of the scores to be roughly independent of $d_{head}$:\n",
    "\n",
    "       $$\n",
    "       \\text{Var}\\Big(\\frac{Q \\cdot K}{\\sqrt{d_{head}}}\\Big) \\approx \\sigma^2\n",
    "       $$\n",
    "\n",
    "   - Why not some other factor?  \n",
    "     - Any other factor would either under- or over-scale the scores.  \n",
    "     - $\\sqrt{d_{head}}$ is the **mathematically correct factor** derived from the fact that a sum of $d_{head}$ independent variables has variance proportional to $d_{head}$.  \n",
    "\n",
    "**Takeaway:**\n",
    "\n",
    "Scaling by $\\sqrt{d_{head}}$ ensures that:\n",
    "\n",
    "- Attention scores remain in a **reasonable range**, regardless of head size.  \n",
    "- Softmax outputs are **neither too sharp nor too flat**.  \n",
    "- Gradients during backpropagation are **stable**, which is critical for training deep transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80be8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Scores (Batch 0, Head 0):\n",
      " [[  0.9   -1.36  -5.84  -7.28   4.87   3.91]\n",
      " [ -0.49  -0.81  -1.67  -5.84   3.58   3.95]\n",
      " [ -4.83  -2.55   0.68 -23.01  13.36  17.55]\n",
      " [ -4.52  -3.78  -3.73 -30.35  18.12  21.89]\n",
      " [  3.4   -0.83  -8.65  -0.33   1.11  -2.09]\n",
      " [  2.12   0.22  -3.07   4.46  -2.28  -4.2 ]]\n"
     ]
    }
   ],
   "source": [
    "# d_head is 2 in our example\n",
    "scaling_factor = np.sqrt(d_head)\n",
    "scaled_scores = scores / scaling_factor\n",
    "\n",
    "print(f\"Scaled Scores (Batch 0, Head 0):\\n\", scaled_scores[0, 0].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af64f03",
   "metadata": {},
   "source": [
    "##### 5.3: Softmax – Converting Raw Scores into Attention Weights\n",
    "This converts the raw scores into probabilities. Each row in the $6 \\times 6$ matrix will now sum to 1. This tells the model: \"For word $i$, how much percentage of attention should I give to word $j$?\"\n",
    "\n",
    "\n",
    "#### more explanation:\n",
    "\n",
    "After computing the **raw attention scores**:\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\frac{Q \\cdot K^T}{\\sqrt{d_{head}}} \\quad \\text{(shape: } B, H, S, S \\text{)}\n",
    "$$\n",
    "\n",
    "we apply the **softmax function** to convert them into probabilities, which become the **attention weights**.\n",
    "\n",
    "---\n",
    "\n",
    "**Softmax formula (for each row vector):**\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "- Here, $x_i$ represents the raw score of a single query token attending to all key tokens.  \n",
    "- After softmax, **each row sums to 1**, representing a valid probability distribution across tokens.  \n",
    "- This answers the question: “For word $i$, how much attention should I give to word $j$?”\n",
    "\n",
    "---\n",
    "\n",
    "**Why subtract the max value for numerical stability?**\n",
    "\n",
    "In code:\n",
    "\n",
    "```python\n",
    "e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "```\n",
    "**Problem:** Exponentials grow very fast. Large numbers in \\(x\\) (e.g., 1000) can cause **overflow** in `np.exp(x)`.\n",
    "\n",
    "**Solution:** Subtract the maximum value of each row before exponentiating. This ensures the largest value in the row becomes 0, keeping exponentials within a **safe numerical range**.\n",
    "\n",
    "**Mathematically, this does not change the softmax result:**\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \n",
    "= \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08b9f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Shape: (2, 2, 6, 6)\n",
      "Attention Weights (Row sum should be 1.0):\n",
      " [[0.01 0.   0.   0.   0.71 0.27]\n",
      " [0.01 0.   0.   0.   0.4  0.58]\n",
      " [0.   0.   0.   0.   0.01 0.99]\n",
      " [0.   0.   0.   0.   0.02 0.98]\n",
      " [0.87 0.01 0.   0.02 0.09 0.  ]\n",
      " [0.09 0.01 0.   0.9  0.   0.  ]]\n",
      "\n",
      "Sum of first row: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # subtracting max for numerical stabiltiy (prevents overflow)\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x/e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "weights = softmax(scaled_scores)\n",
    "\n",
    "print(f\"Weights Shape: {weights.shape}\")\n",
    "print(\"Attention Weights (Row sum should be 1.0):\\n\", weights[0, 0].round(2))\n",
    "print(\"\\nSum of first row:\", np.sum(weights[0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871717f",
   "metadata": {},
   "source": [
    "#### 5.2 Applying Weights to Values\n",
    "Now we use those probabilities to take a weighted sum of the Value vectors ($V$).Math: $Weights \\cdot V$Shape: $(B, H, S, S) \\times (B, H, S, d_{head}) \\rightarrow (B, H, S, d_{head})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "323a2bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output shpae: (2, 2, 6, 2)\n",
      "Context Vectors:\n",
      " [[[[ 2.14402292  0.39960117]\n",
      "   [ 2.67248838  0.49704621]\n",
      "   [ 3.3800271   0.63468213]\n",
      "   [ 3.36749812  0.6325333 ]\n",
      "   [-0.61925828 -1.69258463]\n",
      "   [-2.84382125 -4.2797494 ]]\n",
      "\n",
      "  [[-2.01801767 -1.62729909]\n",
      "   [-0.56197836 -0.9495289 ]\n",
      "   [-0.60145552  1.72734375]\n",
      "   [-2.36034517 -1.62563163]\n",
      "   [-0.61218319 -1.06819435]\n",
      "   [ 0.70400426  4.73762408]]]\n",
      "\n",
      "\n",
      " [[[-0.25085677 -2.23087072]\n",
      "   [-1.00781468 -1.51982567]\n",
      "   [-4.76704565  3.07631155]\n",
      "   [-3.52532198  1.49825583]\n",
      "   [ 0.43357436  1.2195325 ]\n",
      "   [ 1.7634395  -0.22815464]]\n",
      "\n",
      "  [[-0.27479377 -1.13175258]\n",
      "   [-0.01637377 -0.4856992 ]\n",
      "   [-0.28047406 -1.7318373 ]\n",
      "   [-0.06779337 -0.73057099]\n",
      "   [-0.2201747  -1.86373055]\n",
      "   [-0.49290634 -1.46183568]]]]\n",
      "Context Vector for Batch 0, Head 0, Word 0:\n",
      " [2.14402292 0.39960117]\n"
     ]
    }
   ],
   "source": [
    "# this creates the 'context vectors'\n",
    "# (2,2,6,6) @ (2,2,6,2) -> (2,2,6,2)\n",
    "attention_output = np.matmul(weights, V)\n",
    "print(f\"Attention Output shpae: {attention_output.shape}\")\n",
    "print(\"Context Vectors:\\n\", attention_output)\n",
    "print(\"Context Vector for Batch 0, Head 0, Word 0:\\n\", attention_output[0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f71c3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q shape (per head): (2, 2, 6, 2)\n",
      "Final Output shape (per head): (2, 2, 6, 2)\n",
      "\n",
      "Notice how the S (6) and d_head (2) dimensions are perfectly preserved.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial Q shape (per head): {Q.shape}\")\n",
    "print(f\"Final Output shape (per head): {attention_output.shape}\")\n",
    "print(\"\\nNotice how the S (6) and d_head (2) dimensions are perfectly preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70630c37",
   "metadata": {},
   "source": [
    "#### 5.6Concatenation and Final Projection\n",
    "Finally, we merge the heads back together. We want to return to the original shape $(B, S, N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22fadaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after transpose: (2, 6, 2, 2)\n",
      "Final Output after concatenation shape: (2, 6, 4)\n",
      "\n",
      "Concatenated vector for Batch 0, Word 0:\n",
      " [ 2.14402292  0.39960117 -2.01801767 -1.62729909]\n",
      "\n",
      " full concatenated output:\n",
      " [[[ 2.14402292  0.39960117 -2.01801767 -1.62729909]\n",
      "  [ 2.67248838  0.49704621 -0.56197836 -0.9495289 ]\n",
      "  [ 3.3800271   0.63468213 -0.60145552  1.72734375]\n",
      "  [ 3.36749812  0.6325333  -2.36034517 -1.62563163]\n",
      "  [-0.61925828 -1.69258463 -0.61218319 -1.06819435]\n",
      "  [-2.84382125 -4.2797494   0.70400426  4.73762408]]\n",
      "\n",
      " [[-0.25085677 -2.23087072 -0.27479377 -1.13175258]\n",
      "  [-1.00781468 -1.51982567 -0.01637377 -0.4856992 ]\n",
      "  [-4.76704565  3.07631155 -0.28047406 -1.7318373 ]\n",
      "  [-3.52532198  1.49825583 -0.06779337 -0.73057099]\n",
      "  [ 0.43357436  1.2195325  -0.2201747  -1.86373055]\n",
      "  [ 1.7634395  -0.22815464 -0.49290634 -1.46183568]]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Tranpose: (B, H, S, d_head) -> (B, S, H, d_head)\n",
    "# we move the sequence dimension back to second position so heads are side-by-side for each word\n",
    "out_tranposed = attention_output.transpose(0, 2, 1, 3)  # (B, S, H, d_head)\n",
    "print(f\"Shape after transpose: {out_tranposed.shape}\") # we get (2,6,2,2)\n",
    "\n",
    "# 2. reshape: (B, S, H, d_head) -> (B, S, H*d_head) = (B, S, N)\n",
    "out_concat = out_tranposed.reshape(B, S, N)\n",
    "\n",
    "print(f\"Final Output after concatenation shape: {out_concat.shape}\")\n",
    "print(\"\\nConcatenated vector for Batch 0, Word 0:\\n\", out_concat[0, 0])\n",
    "print(\"\\n full concatenated output:\\n\", out_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430893b",
   "metadata": {},
   "source": [
    "###  Summary of Matrix Transformations\n",
    "\n",
    "| Step        | Operation              | Resulting Shape / Shape Evolution           | Logic / Explanation                          |\n",
    "|------------|------------------------|--------------------------------------------|---------------------------------------------|\n",
    "| Input       | X                     | (2, 6, 4)                                  | Original input embeddings                    |\n",
    "| Project     | X W_q                 | (2, 6, 4)                                  | Linear projection to Queries (similarly K, V) |\n",
    "| Split       | Reshape & Transpose   | (2, 2, 6, 2)                               | Split into heads and prepare for attention  |\n",
    "| Attention   | Q @ Kᵀ                | (2, 2, 6, 6)                               | Word-to-word similarity map                  |\n",
    "| Scaling     | / √d_head             | (2, 2, 6, 6)                               | Keep values small                            |\n",
    "| Softmax     | P(x)                  | (2, 2, 6, 6)                               | Normalized attention weights                 |\n",
    "| Context     | Scores ⋅ V            | (2, 2, 6, 2)                               | The \"contextualized\" word vectors           |\n",
    "| Merge       | Transpose & Reshape   | (2, 6, 4)                                  | Combine heads back into original embedding size |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f370f593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
