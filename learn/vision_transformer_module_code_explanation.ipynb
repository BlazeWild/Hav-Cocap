{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30606a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution:int, patch_size:int, width:int, layers:int, heads:int, output_dim:int):\n",
    "        \"\"\"class VisionTransformer(nn.Module):\n",
    "\n",
    "        Vision Transformer (ViT) for image classification.\n",
    "\n",
    "        Splits the input image into patches, projects each patch into a `width`-dim embedding, prepends a \n",
    "        learnable class token, adds learnable positional embeddings, and passes the sequence through a \n",
    "        Transformer encoder. The output of the class token is used as the global image representation.\n",
    "\n",
    "        Args:\n",
    "            input_resolution (int): Input image size (assumed square), e.g., 224 for 224x224 images.\n",
    "            patch_size (int): Size of each square patch, e.g., 16.\n",
    "            width (int): Embedding dimension for patches and class token (d_model), e.g., 768.\n",
    "            layers (int): Number of Transformer blocks to stack.\n",
    "            heads (int): Number of attention heads per block.\n",
    "            output_dim (int): Dimension of final output (e.g., number of classes).\n",
    "            in_channels (int, optional): Number of image channels (default 3 for RGB).\n",
    "\n",
    "        Notes:\n",
    "            - class_embedding is learnable and summarizes the image.It is same for all images initially as it is randomly initialized.After training it learns to be a good summary of the image.And is different for different images.\n",
    "            Eg. Sequence for one image: [Class_embedding, patch1, patch2, patch3, patch4]\n",
    "            - positional_embedding is learnable and encodes spatial info.\n",
    "            - conv1 splits image into patches and projects them.\n",
    "            - scale = width**-0.5 ensures stable initialization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "        \n",
    "        scale = width ** -0.5 #eg, for dim=768, scale = 1/sqrt(768)=0.036\n",
    "        #initializing embeddings with huge random numbers (e.g. N(0,1)),we shrink them so their average magnitude ‚âà 0.036 ‚Üí stable dot products, stable softmax.\n",
    "        #class embedding is a learnable parameter that is added to the sequence of patch embeddings to represent the entire image\n",
    "        self.class_embedding = nn.Parameter(scale*torch.randn(width)) # a random vector of size width\n",
    "        # learnable positional embeddings for each patch + 1 for class embedding\n",
    "        self.positional_embedding = nn.Parameter(scale*torch.randn((input_resolution // patch_size) ** 2 + 1, width)) # as it is learnable, we write parameter when imnitializing, so that it can be updated during training.\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "        \n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "        \n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale*torch.randn(width, output_dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, output_all_features: bool = False, output_attention_map: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input images of shape [batch_size, in_channels, height, width].\n",
    "            output_all_features (bool, optional): If True, the function will also return all patch embeddings, not just the class token. Useful for things like visualizing patch features or doing segmentation tasks. Default: False.\n",
    "            output_attention_map (bool, optional): If True, the function will return attention maps from the class token to all patches. Useful for visualizing where the model ‚Äúlooks‚Äù in the image.Default: False.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains at least the class token features (cls_feature) of shape [batch_size, output_dim].\n",
    "                Optionally:\n",
    "                - Patch embeddings of shape [batch_size, num_patches, width] (if output_all_features=True)\n",
    "                - Attention maps of shape [n_layers, batch_size, n_heads, grid, grid] (if output_attention_map=True)\n",
    "\n",
    "        Notes:\n",
    "            - Images are first converted to patch embeddings via a Conv2d layer.\n",
    "            - A learnable class token is prepended to the sequence to aggregate image-level information.\n",
    "            - Learnable positional embeddings are added to each token (including the class token).\n",
    "            - The sequence is normalized (LayerNorm) and passed through the Transformer blocks.\n",
    "            - The class token embedding is extracted, normalized, and projected to output_dim for downstream tasks.\n",
    "            - Patch embeddings and attention maps are optional outputs useful for visualization or analysis.\n",
    "\n",
    "            Input image: [B, 3, 224, 224]\n",
    "                    ‚îÇ\n",
    "            Conv2d ‚Üí Patch embeddings: [B, 768, 14, 14]\n",
    "                    ‚îÇ\n",
    "            Flatten ‚Üí [B, 196, 768]\n",
    "                    ‚îÇ\n",
    "            Add CLS token ‚Üí [B, 197, 768]\n",
    "                    ‚îÇ\n",
    "            Add positional embeddings ‚Üí [B, 197, 768]\n",
    "                    ‚îÇ\n",
    "            LayerNorm ‚Üí [B, 197, 768]\n",
    "                    ‚îÇ\n",
    "            Transformer ‚Üí [B, 197, 768], attn maps [layers, B, heads, 197, 197]\n",
    "                    ‚îÇ\n",
    "            Extract CLS token ‚Üí [B, output_dim]\n",
    "                    ‚îÇ\n",
    "            Optional outputs ‚Üí patch embeddings, attention maps\n",
    "\n",
    "        \"\"\"\n",
    "        #split image into non-overlapping patches and project to `width` dimensions\n",
    "        x = self.conv1(x) #shape=[*, width, grid, grid], eg. [*, 768, 14, 14] for 224x224 input and 16x16 patches\n",
    "        grid = x.size(2)\n",
    "        #flatten the 2D grid into a sequence of patches\n",
    "        x = x.reshape(x.shape[0], x.shape[1],-1) #shape=[*, width, grid**2] eg. [*, 768, 196]\n",
    "        x = x.permute(0,2,1) #shape=[*, grid**2, width]\n",
    "        #add class token to the beginning of the sequence\n",
    "        # self.class_embedding has shape (width,) ‚Üí 1D vector, this exmplanation is written in the vision_transformer_explanation.ipynb file with exmaple\n",
    "        batch_class_token = self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x=torch.cat(\n",
    "            [batch_class_token, x],\n",
    "             dim=1) #shape=[*, grid**2+1, width]\n",
    "        x=x + self.positional_embedding.to(x.dtype) #add positional embeddings\n",
    "        \n",
    "        #pre normalize all sequqnce elements including class token before feeding to the transfprmer though there is ln_1 (layernorm) in each block of transformer as the ln_1 normalizes per block, so both are needed\n",
    "        x=self.ln_pre(x)\n",
    "        # as transformer expects input of shape (seq_len, batch, width)\n",
    "        x.permute(1,0,2) #NLD-> LND , shape=[grid**2+1, *, width] ,(* is batch size)\n",
    "        x,attn = self.transformer(x) #shape=[grid**2+1, *, width], attn shape=[layers, *, heads, grid**2+1, grid**2+1]\n",
    "        x = x.permute(1,0,2) #LND-> NLD\n",
    "        \n",
    "        \n",
    "        \n",
    "        # this is for class_feature extraction and is used for feature extraction or classification, ...It is not strictly needed. but in the cocap, it is included for feature extraction\n",
    "        # ln_post normalizes the class token embedding\n",
    "        # @ self.proj projects the embedding from width -> output_dim. as it is matrix multiplication\n",
    "        # Shape after projection: [batch_size, output_dim]\n",
    "        #x[:, 0, :] ‚Üí selects the class token embedding for all images in the batch.\n",
    "        cls_feature = self.ln_post(x[:,0,:]) @ self.proj ## cls_feature.shape = [batch_size, output_dim]\n",
    "        \n",
    "        # 1Ô∏è‚É£1Ô∏è‚É£ Prepare outputs tuple\n",
    "        # Start with just the class token feature as primary output\n",
    "        outputs = (cls_feature,)\n",
    "        \n",
    "        # Optional: include patch embeddings\n",
    "        if output_all_features:\n",
    "            # x[:, 1:, :] excludes the class token and keeps only patch embeddings\n",
    "            # Shape: [batch_size, num_patches, width]\n",
    "            outputs += (x[:, 1:, :],)\n",
    "            # Purpose:\n",
    "            # - Useful for tasks where individual patch features are needed\n",
    "            #   e.g., segmentation, attention visualization, or feature extraction\n",
    "\n",
    "        # Optional: include attention maps\n",
    "        if output_attention_map:\n",
    "            # attn.shape = [n_layers, batch_size, n_heads, seq_len, seq_len]\n",
    "            # attn[:, :, :, 0, 1:] selects attention from the class token to all patches\n",
    "            # Shape: [n_layers, batch_size, n_heads, num_patches]\n",
    "            # einops rearranges it to match the 2D grid layout of patches: [n_layers, batch_size, n_heads, h, w]\n",
    "            outputs += (einops.rearrange(\n",
    "                attn[:, :, :, 0, 1:],  # class token attends to patches\n",
    "                \"n_layers b n_heads (h w) -> n_layers b n_heads h w\",\n",
    "                h=grid, w=grid\n",
    "            ),)\n",
    "            # Purpose:\n",
    "            # - Visualizes where the class token \"looks\" in the image\n",
    "            # - Helpful for interpretability of attention\n",
    "\n",
    "        # 1Ô∏è‚É£2Ô∏è‚É£ Return final outputs\n",
    "        # Tuple contains:\n",
    "        # 1. cls_feature: image-level representation for classification\n",
    "        # 2. (optional) patch embeddings: individual patch features\n",
    "        # 3. (optional) attention maps: visualization of attention from class token to patches\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9c53e",
   "metadata": {},
   "source": [
    "## EXPLAINING ABOVE CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98caad26",
   "metadata": {},
   "source": [
    "Ahhh okay, now I **get exactly what‚Äôs confusing you** üòÖ ‚Äî you‚Äôre asking **not what the outputs are**, but **where do they come from inside the network**, i.e., the **flow of tensors through the blocks** that produces `cls_feature`, `patch_features`, and `attention_map`. Let‚Äôs go **step by step from input to output**, including which intermediate nodes produce them.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 0Ô∏è‚É£ Input image\n",
    "\n",
    "```python\n",
    "x: [batch, in_channels, H, W] ‚Üí e.g., [8, 3, 224, 224]\n",
    "```\n",
    "\n",
    "* This is your raw image.\n",
    "* Goes into `self.conv1` to turn into **patch embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1Ô∏è‚É£ Patch embedding (`x` after conv1)\n",
    "\n",
    "```python\n",
    "x = self.conv1(x)  # shape: [B, width, grid, grid] ‚Üí e.g., [8, 768, 14, 14]\n",
    "```\n",
    "\n",
    "* Each patch becomes a `width`-dimensional embedding vector.\n",
    "* Output: **patch-level features** (still 2D grid at this stage).\n",
    "* Not yet a sequence for transformer.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 2Ô∏è‚É£ Flatten grid ‚Üí sequence\n",
    "\n",
    "```python\n",
    "x = x.reshape(B, width, grid*grid)  # [8, 768, 196]\n",
    "x = x.permute(0, 2, 1)             # [B, num_patches, width] ‚Üí [8, 196, 768]\n",
    "```\n",
    "\n",
    "* Each patch is now **one token in a sequence**.\n",
    "* This is **the sequence input to the transformer**, before adding class token.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 3Ô∏è‚É£ Add CLS token\n",
    "\n",
    "```python\n",
    "batch_class_token = self.class_embedding.to(x.dtype) + torch.zeros(B, 1, width)\n",
    "x = torch.cat([batch_class_token, x], dim=1)\n",
    "```\n",
    "\n",
    "* `self.class_embedding` ‚Üí **learnable tensor** `[1, width]`\n",
    "* Broadcasted to `[B, 1, width]`\n",
    "* Concatenated **at the start of the sequence** ‚Üí `[B, num_patches+1, width]` ‚Üí `[8, 197, 768]`\n",
    "\n",
    "‚úÖ Now the **CLS token is part of the input sequence**, not separate.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 4Ô∏è‚É£ Add positional embeddings\n",
    "\n",
    "```python\n",
    "x = x + self.positional_embedding.to(x.dtype)\n",
    "```\n",
    "\n",
    "* Shape: `[B, 197, 768]`\n",
    "* Each token (CLS + patches) gets a **position vector**.\n",
    "* Still part of the same sequence.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 5Ô∏è‚É£ Pre-normalize\n",
    "\n",
    "```python\n",
    "x = self.ln_pre(x)\n",
    "```\n",
    "\n",
    "* Normalizes each token vector\n",
    "* Output shape: `[B, 197, 768]`\n",
    "* Still **sequence of tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 6Ô∏è‚É£ Transformer blocks\n",
    "\n",
    "```python\n",
    "x.permute(1,0,2)          # [seq_len, B, width] ‚Üí [197, 8, 768]\n",
    "x, attn = self.transformer(x)  \n",
    "x = x.permute(1,0,2)      # back to [B, 197, 768]\n",
    "```\n",
    "\n",
    "### What happens here:\n",
    "\n",
    "* `self.transformer` is a stack of **multi-head attention + feed-forward blocks**\n",
    "* Input: `[CLS + patch tokens]` sequence\n",
    "* Output:\n",
    "\n",
    "  * `x` ‚Üí **same shape `[B, 197, 768]`**, updated embeddings for each token\n",
    "\n",
    "    * `x[:,0,:]` ‚Üí **CLS token embedding**\n",
    "    * `x[:,1:,:]` ‚Üí **patch embeddings**\n",
    "  * `attn` ‚Üí attention matrices `[n_layers, B, n_heads, seq_len, seq_len]`\n",
    "\n",
    "‚úÖ **Key point:** CLS token is **not injected separately**, it travels **through the transformer as part of the sequence**, learning to ‚Äúaggregate‚Äù information from all patches.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 7Ô∏è‚É£ Extract CLS token ‚Üí `cls_feature`\n",
    "\n",
    "```python\n",
    "cls_feature = self.ln_post(x[:,0,:]) @ self.proj\n",
    "```\n",
    "\n",
    "* `x[:,0,:]` ‚Üí **first token** (CLS) after transformer ‚Üí `[B, width]`\n",
    "* LayerNorm ‚Üí normalize\n",
    "* Linear projection ‚Üí `[B, output_dim]`\n",
    "\n",
    "üí° This is why **`cls_feature` comes from the previous node**, specifically:\n",
    "\n",
    "```\n",
    "x (output of transformer) ‚Üí select [:,0,:] ‚Üí ln_post ‚Üí proj\n",
    "```\n",
    "\n",
    "* Not injected externally. CLS token **propagates with sequence** and transforms.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Step 8Ô∏è‚É£ Optional: Patch embeddings ‚Üí `output_all_features`\n",
    "\n",
    "```python\n",
    "if output_all_features:\n",
    "    outputs += (x[:, 1:, :],)\n",
    "```\n",
    "\n",
    "* **Source:** transformer output sequence `x`\n",
    "\n",
    "  * `x[:,1:,:]` ‚Üí all tokens **except CLS token**\n",
    "* **Shape:** `[B, num_patches, width]` ‚Üí e.g., `[8, 196, 768]`\n",
    "* **Purpose / Why use:**\n",
    "\n",
    "  * Gives **individual patch-level embeddings**\n",
    "  * Useful for tasks that require **per-patch information**, e.g.:\n",
    "\n",
    "    * Segmentation\n",
    "    * Patch-level feature extraction\n",
    "    * Attention visualization per patch\n",
    "* **Why we don‚Äôt always need it:**\n",
    "\n",
    "  * For **image-level classification**, the **CLS token already aggregates all patch information**\n",
    "  * Returning all patch embeddings increases memory footprint unnecessarily\n",
    "\n",
    "‚úÖ So this output is optional ‚Äî only included if `output_all_features=True`.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 9Ô∏è‚É£ Optional: Attention maps ‚Üí `output_attention_map`\n",
    "\n",
    "```python\n",
    "if output_attention_map:\n",
    "    outputs += (einops.rearrange(\n",
    "        attn[:, :, :, 0, 1:],  # CLS token attends to all patches\n",
    "        \"n_layers b n_heads (h w) -> n_layers b n_heads h w\",\n",
    "        h=grid, w=grid\n",
    "    ),)\n",
    "```\n",
    "\n",
    "* **Source:** `attn` tensor from transformer blocks\n",
    "\n",
    "  * `attn` shape: `[n_layers, B, n_heads, seq_len, seq_len]` ‚Üí `[12, 8, 12, 197, 197]` for example\n",
    "  * Slice `[:, :, :, 0, 1:]` ‚Üí **only CLS token attending to patches**\n",
    "  * Shape after slice: `[n_layers, B, n_heads, num_patches]` ‚Üí `[12, 8, 12, 196]`\n",
    "  * Reshaped to `[n_layers, B, n_heads, grid, grid]` ‚Üí `[12, 8, 12, 14, 14]` for visualization\n",
    "* **Purpose / Why use:**\n",
    "\n",
    "  * Visualizes **where the CLS token is ‚Äúlooking‚Äù** in the image\n",
    "  * Helps **interpretability** ‚Äî e.g., which patches the model focuses on for classification\n",
    "* **Why we don‚Äôt need all attention:**\n",
    "\n",
    "  * Full attention tensor `[n_layers, B, n_heads, seq_len, seq_len]` contains **patch ‚Üí patch attention**\n",
    "  * Patch ‚Üí patch attention is mostly **internal information**, rarely needed for analysis or downstream tasks\n",
    "  * Returning only CLS ‚Üí patches gives **meaningful attention heatmaps** and is **much smaller**\n",
    "  * `attn[:, :, :, 0, 1:]` ‚Üí takes **all layers, all batches, all heads**, but only the **CLS token (row 0) attending to all patches (columns 1 onward)**\n",
    "  * This represents **how the CLS token ‚Äúlooks at‚Äù each patch** in the image, across the model hierarchy and attention heads\n",
    "  * We don‚Äôt take full attention (`seq_len √ó seq_len`) because **patch ‚Üí patch attention** is mostly **internal** and not needed for visualization or classification\n",
    "  * This slice is **small, meaningful, and directly interpretable** as a heatmap over the image patches\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "‚úÖ Optional output, included only if `output_attention_map=True`.\n",
    "\n",
    "---\n",
    "\n",
    "# Step 10Ô∏è‚É£ Return final outputs\n",
    "\n",
    "```python\n",
    "return outputs\n",
    "```\n",
    "\n",
    "* **Tuple contains:**\n",
    "\n",
    "  1. `cls_feature` ‚Üí `[B, output_dim]` ‚Üí main image representation\n",
    "  2. `(optional) patch_features` ‚Üí `[B, num_patches, width]` ‚Üí only if `output_all_features=True`\n",
    "  3. `(optional) attention_map` ‚Üí `[n_layers, B, n_heads, grid, grid]` ‚Üí only if `output_attention_map=True`\n",
    "\n",
    "* **Key insight:**\n",
    "\n",
    "  * CLS token is **central** ‚Äî aggregates all information\n",
    "  * Patch embeddings and attention maps are **slices of the same transformer output**\n",
    "  * They are optional because **for most classification tasks, CLS token alone is sufficient**\n",
    "  * Returning the others is useful for **visualization, research, or patch-level tasks**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can now **draw a full diagram showing the CLS token + patches flowing through the transformer, and exactly where `cls_feature`, `patch_features`, and `attention_map` come from**, including their **shapes at each step**.\n",
    "\n",
    "Do you want me to do that next?\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Summary of ‚Äúwhere each output comes from‚Äù\n",
    "\n",
    "| Output         | Source inside forward                               | Shape                                | Notes                                                         |\n",
    "| -------------- | --------------------------------------------------- | ------------------------------------ | ------------------------------------------------------------- |\n",
    "| cls_feature    | `x[:,0,:]` from transformer output ‚Üí ln_post ‚Üí proj | `[B, output_dim]`                    | Image-level representation, learned via CLS token aggregation |\n",
    "| patch_features | `x[:,1:,:]` from transformer output                 | `[B, num_patches, width]`            | Each patch embedding after transformer                        |\n",
    "| attention_map  | `attn[:, :, :, 0, 1:]` from transformer attention   | `[n_layers, B, n_heads, grid, grid]` | CLS token attention to patches, reshaped                      |\n",
    "\n",
    "‚úÖ **Key insight:** CLS token is **part of the input sequence**, travels through transformer, learns to aggregate patches ‚Üí `cls_feature`. Patch embeddings and attention maps are **slices or transformations** of the **same transformer output**.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **draw a diagram showing CLS token + patch tokens flowing through transformer, and exactly where cls_feature, patch_features, and attention maps are extracted**, which makes this super obvious visually.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8315a32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
