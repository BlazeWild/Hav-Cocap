{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dcd6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c085f91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3100, -0.8800, -0.1000, -0.8100],\n",
       "         [ 1.2000, -0.1900, -2.4200, -0.6300],\n",
       "         [ 0.3900, -0.5800, -0.3900,  0.8000],\n",
       "         [-1.4700,  0.7700,  0.2000,  0.7900],\n",
       "         [-1.9700,  0.4300,  1.1700,  0.4100],\n",
       "         [-1.0100,  0.0800, -0.1700, -0.5400]],\n",
       "\n",
       "        [[ 0.6400,  1.5200,  0.0700, -1.0900],\n",
       "         [-0.0500,  2.1200,  1.2900,  0.3000],\n",
       "         [-0.8300,  1.2300,  0.6700,  0.8800],\n",
       "         [-0.6000, -4.2100, -1.8400,  0.9100],\n",
       "         [ 1.4800,  1.6800, -0.7000,  0.0600],\n",
       "         [-0.4900, -1.7600, -1.0200, -1.0100]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Setup Hyperparameters\n",
    "B, S, N, H = 2, 6, 4, 2 ## Batch, Seq, Features (d_model), Heads\n",
    "\n",
    "# 2. Define Input X\n",
    "X = torch.randn(B, S, N, dtype=torch.float32).round(decimals=2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27fb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Weight Matrices (from Cell 24)\n",
    "# Note: We transpose (.T) because PyTorch weights are (out_features, in_features)\n",
    "W_q = torch.tensor([[ 0.1, 0.27, -0.35, -0.11], [-1.86, -0.25, -1.25, -0.36], [-0.71, 1.71, -1.17, 1.05], [ 0.08, 1.76, 0.53, -0.1 ]]).T\n",
    "W_k = torch.tensor([[ 0.7, -1.52, 2.67, 0.06], [-1.46, 2.02, -0.78, -0.83], [ 1.39, -1.07, -0.52, 1.11], [ 1.73, 1.05, 1.47, 1.17 ]]).T\n",
    "W_v = torch.tensor([[-1.26, 0.25, 0.03, 1.92], [ 0.58, -0.25, -0.46, 1.69], [ 0.08, -1.85, -0.71, 0.3 ], [ 1.64, -0.9, 1.04, 0.72 ]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4bfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Initialize the PyTorch Layer\n",
    "# batch_first=True makes it (B, S, N)\n",
    "mha_layer = nn.MultiheadAttention(embed_dim=N,num_heads=H,batch_first=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "031e9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Res: (tensor([[[-4.1946,  1.5485,  0.1938, -0.4855],\n",
      "         [-4.7574,  1.7115,  1.0208,  0.9742],\n",
      "         [ 2.9631, -1.8571,  0.0343,  1.9806],\n",
      "         [ 1.1111,  0.7130, -0.1266, -1.4250],\n",
      "         [ 3.2098, -1.3483, -0.5693, -2.2644],\n",
      "         [-4.7355,  1.7198,  0.4110,  0.4292]],\n",
      "\n",
      "        [[-1.8944,  1.7517, -0.8658,  2.2282],\n",
      "         [-1.6384,  0.6529, -0.5742,  2.0830],\n",
      "         [-0.8443,  0.1915, -0.7233,  1.8812],\n",
      "         [-0.3406,  3.4875,  4.0624, -7.8203],\n",
      "         [-1.7342,  1.0133, -1.4312,  2.2687],\n",
      "         [-0.3406,  3.4875,  2.9986, -4.4684]]], grad_fn=<TransposeBackward0>), tensor([[[5.2302e-01, 7.2577e-02, 1.3457e-01, 8.9100e-02, 9.4946e-02,\n",
      "          8.5791e-02],\n",
      "         [5.5581e-01, 4.4412e-01, 6.1800e-05, 5.5316e-09, 3.4390e-11,\n",
      "          6.4870e-08],\n",
      "         [3.9126e-01, 9.4588e-02, 1.0231e-01, 2.1267e-01, 1.9389e-01,\n",
      "          5.2844e-03],\n",
      "         [2.1251e-02, 1.7069e-01, 3.6781e-02, 3.4365e-01, 2.0563e-01,\n",
      "          2.2199e-01],\n",
      "         [3.9623e-05, 1.5830e-02, 8.6481e-04, 4.7873e-01, 4.6117e-01,\n",
      "          4.3369e-02],\n",
      "         [6.4023e-01, 1.2526e-01, 1.0022e-01, 4.7863e-02, 3.6305e-02,\n",
      "          5.0128e-02]],\n",
      "\n",
      "        [[3.3658e-01, 1.8272e-01, 2.2540e-01, 2.9133e-05, 4.3263e-03,\n",
      "          2.5094e-01],\n",
      "         [4.7075e-01, 1.4631e-01, 3.4705e-01, 1.8129e-08, 3.1679e-02,\n",
      "          4.2139e-03],\n",
      "         [3.3433e-01, 2.2566e-01, 2.7209e-01, 3.5786e-04, 1.2597e-01,\n",
      "          4.1594e-02],\n",
      "         [4.7297e-13, 1.2609e-15, 1.0151e-15, 9.8745e-01, 1.2552e-02,\n",
      "          1.0673e-12],\n",
      "         [7.3573e-01, 7.5483e-02, 4.7340e-02, 1.8622e-05, 1.9886e-02,\n",
      "          1.2155e-01],\n",
      "         [1.7788e-05, 1.5839e-06, 8.7498e-07, 8.6490e-01, 1.3506e-01,\n",
      "          2.1217e-05]]], grad_fn=<MeanBackward1>))\n",
      "=== ATTENTION_RES[0] (Concatenated Output) ===\n",
      "Shape: torch.Size([2, 6, 4])\n",
      "tensor([[[-4.1946,  1.5485,  0.1938, -0.4855],\n",
      "         [-4.7574,  1.7115,  1.0208,  0.9742],\n",
      "         [ 2.9631, -1.8571,  0.0343,  1.9806],\n",
      "         [ 1.1111,  0.7130, -0.1266, -1.4250],\n",
      "         [ 3.2098, -1.3483, -0.5693, -2.2644],\n",
      "         [-4.7355,  1.7198,  0.4110,  0.4292]],\n",
      "\n",
      "        [[-1.8944,  1.7517, -0.8658,  2.2282],\n",
      "         [-1.6384,  0.6529, -0.5742,  2.0830],\n",
      "         [-0.8443,  0.1915, -0.7233,  1.8812],\n",
      "         [-0.3406,  3.4875,  4.0624, -7.8203],\n",
      "         [-1.7342,  1.0133, -1.4312,  2.2687],\n",
      "         [-0.3406,  3.4875,  2.9986, -4.4684]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "=== ATTENTION_RES[1] (Weight Matrix) ===\n",
      "Shape: torch.Size([2, 6, 6])\n",
      "tensor([[[5.2302e-01, 7.2577e-02, 1.3457e-01, 8.9100e-02, 9.4946e-02,\n",
      "          8.5791e-02],\n",
      "         [5.5581e-01, 4.4412e-01, 6.1800e-05, 5.5316e-09, 3.4390e-11,\n",
      "          6.4870e-08],\n",
      "         [3.9126e-01, 9.4588e-02, 1.0231e-01, 2.1267e-01, 1.9389e-01,\n",
      "          5.2844e-03],\n",
      "         [2.1251e-02, 1.7069e-01, 3.6781e-02, 3.4365e-01, 2.0563e-01,\n",
      "          2.2199e-01],\n",
      "         [3.9623e-05, 1.5830e-02, 8.6481e-04, 4.7873e-01, 4.6117e-01,\n",
      "          4.3369e-02],\n",
      "         [6.4023e-01, 1.2526e-01, 1.0022e-01, 4.7863e-02, 3.6305e-02,\n",
      "          5.0128e-02]],\n",
      "\n",
      "        [[3.3658e-01, 1.8272e-01, 2.2540e-01, 2.9133e-05, 4.3263e-03,\n",
      "          2.5094e-01],\n",
      "         [4.7075e-01, 1.4631e-01, 3.4705e-01, 1.8129e-08, 3.1679e-02,\n",
      "          4.2139e-03],\n",
      "         [3.3433e-01, 2.2566e-01, 2.7209e-01, 3.5786e-04, 1.2597e-01,\n",
      "          4.1594e-02],\n",
      "         [4.7297e-13, 1.2609e-15, 1.0151e-15, 9.8745e-01, 1.2552e-02,\n",
      "          1.0673e-12],\n",
      "         [7.3573e-01, 7.5483e-02, 4.7340e-02, 1.8622e-05, 1.9886e-02,\n",
      "          1.2155e-01],\n",
      "         [1.7788e-05, 1.5839e-06, 8.7498e-07, 8.6490e-01, 1.3506e-01,\n",
      "          2.1217e-05]]], grad_fn=<MeanBackward1>)\n",
      "\n",
      "=== X + ATTENTION_RES[0] (Residual Connection) ===\n",
      "tensor([[[-1.8846,  0.6685,  0.0938, -1.2955],\n",
      "         [-3.5574,  1.5215, -1.3992,  0.3442],\n",
      "         [ 3.3531, -2.4371, -0.3557,  2.7806],\n",
      "         [-0.3589,  1.4830,  0.0734, -0.6350],\n",
      "         [ 1.2398, -0.9183,  0.6007, -1.8544],\n",
      "         [-5.7455,  1.7998,  0.2410, -0.1108]],\n",
      "\n",
      "        [[-1.2544,  3.2717, -0.7958,  1.1382],\n",
      "         [-1.6884,  2.7729,  0.7158,  2.3830],\n",
      "         [-1.6743,  1.4215, -0.0533,  2.7612],\n",
      "         [-0.9406, -0.7225,  2.2224, -6.9103],\n",
      "         [-0.2542,  2.6933, -2.1312,  2.3287],\n",
      "         [-0.8306,  1.7275,  1.9786, -5.4784]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 5. Manually Load Weights\n",
    "# PyTorch combines Q,K,V weights into one \"in_proj_weight\" tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    mha_layer.in_proj_weight.copy_(torch.cat([W_q, W_k, W_v]))\n",
    "    # set output projection to identity to match notebook logic\n",
    "    mha_layer.out_proj.weight.copy_(torch.eye(N))\n",
    "\n",
    "\n",
    "# 6. Run the Attention forward pass\n",
    "# returns (output, weights) as tuple\n",
    "attention_res = mha_layer(X, X, X)\n",
    "\n",
    "\n",
    "# --- THE PRINTS YOU WANTED ---\n",
    "\n",
    "print(f\"Attention Res: {attention_res}\")\n",
    "\n",
    "print(\"=== ATTENTION_RES[0] (Concatenated Output) ===\")\n",
    "print(f\"Shape: {attention_res[0].shape}\") # (2, 6, 4)\n",
    "print(attention_res[0]) \n",
    "\n",
    "print(\"\\n=== ATTENTION_RES[1] (Weight Matrix) ===\")\n",
    "print(f\"Shape: {attention_res[1].shape}\") # (2, 6, 6)\n",
    "print(attention_res[1])\n",
    "\n",
    "# This is how your residual line works:\n",
    "x_updated = X + attention_res[0]\n",
    "print(\"\\n=== X + ATTENTION_RES[0] (Residual Connection) ===\")\n",
    "print(x_updated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
